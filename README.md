# Comparing different search strategies on NAS-Bench-101

In recent years, Machine Learning (ML) has seen a huge spike in the use of its models in various application areas such as image recognition, machine translation, and many more. Almost all of these models have multiple layers that are computationally expensive and memory intensive. Not only this, but it also takes tremendous manual efforts to come up with models with optimized parameters that give the best results for a given task. An area that helps here is Automated machine learning (AutoML), which automates the process of applying ML algorithms to real-world problems. Neural Architecture Search (NAS), also considered a subdomain of AutoML, helps automate designing neural networks. NAS-Bench-101 is one such public dataset of architectures for NAS research purposes. We enhanced the strategy of NAS-Bench-101; in addition to random search and regularised evolution strategy, we also explored genetic algorithm and provided a comparative study to understand the different behavior of each search strategy. Based on our results, the genetic algorithm finds more architectures yielding higher accuracy as compared to regularised evolution strategy, which in turn finds more architectures with accuracy relatively higher than architectures found by random search. In the end, to overcome this rigorous training, we used regression to predict the results of architectures based on their graphs attributes.
