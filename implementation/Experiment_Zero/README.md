# Trends of different results while tweaking hyperparameters on the following architecture:

# Architecture (7x7):

[0, 1, 1, 0, 1, 1, 1]

[0, 0, 0, 0, 1, 1, 0]

[0, 0, 0, 1, 1, 1, 1]

[0, 0, 0, 0, 0, 1, 1]

[0, 0, 0, 0, 0, 0, 1]

[0, 0, 0, 0, 0, 0, 0]

[0, 0, 0, 0, 0, 0, 0]


## 1. Results of different optimzers on the following hyperparameters:

**Batch Size:** 256

**Learning Rate:** 0.01

**Epochs:** 50



## 2. Results of different optimzers on the following hyperparameters (increasing number of training epochs):

**Batch Size:** 256

**Learning Rate:** 0.01

**Epochs:** 100



## 3. Results of different optimzers on the following hyperparameters (increasing learning rate):

**Batch Size:** 256

**Learning Rate:** 0.08

**Epochs:** 100



## 4. Results of different optimzers on the following hyperparameters (decreasing batch size):

**Batch Size:** 128

**Learning Rate:** 0.08

**Epochs:** 100



## 5. Results of different optimzers on the following hyperparameters (more decreasing batch size):

**Batch Size:** 32

**Learning Rate:** 0.08

**Epochs:** 100



## 6. Results of implementing adam optimizer on almost minimum batch size and learning rate:

**Batch Size:** 8

**Learning Rate:** 0.01

**Epochs:** 100



## 7. Results of adam optimzer after increasing number of epochs to 200:

**Batch Size:** 32

**Learning Rate:** 0.01

**Epochs:** 200



## 8. Comparing the results of adam optimizers on different batch sizes:

**Optimizer:** Adam

**Learning Rate:** 0.08

**Epochs:** 100
